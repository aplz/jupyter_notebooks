{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "## Read the docs\n",
    "* [TextBlobDE](https://pypi.python.org/pypi/textblob-de)\n",
    "* [spaCy](https://spacy.io/api/doc)\n",
    "\n",
    "## Installation\n",
    "On windows, run the conda shell and issue the following commands to install the required libraries and extensions.\n",
    "\n",
    "### Install spacy\n",
    "1. Basic installation: `conda install -c conda-forge spacy`\n",
    "2. Download the German model: `python -m spacy download de`\n",
    "\n",
    "Make sure to run the shell as adminastrator if you are on a windows machine since otherwise linking will fail.\n",
    "\n",
    "### Install textblob\n",
    "1. Basic installation: `conda install -c conda-forge textblob`\n",
    "2. Download the German extension: `pip install -U textblob-de`\n",
    "3. Fetch models/data/corpora: `python -m textblob.download_corpora`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for handling German text containing umlauts\n",
    "from __future__ import unicode_literals\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "import spacy\n",
    "# Load the German annotation models\n",
    "spacy_nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the first step in any natural language processing task and divides a sequence like a sentence into lexical units or words. Here are some points that render even this basic task challenging:\n",
    "* should we split only on white spaces? No. This would leave us with punctuation glued to the actual words.\n",
    "* should we additionally split on punctuation? Yes, but it depends. \n",
    " * But how many tokens are in \"St. Nick? or \"Dr. Schmitz\"? Two or three? And what about \"e.g.\", \"i.e.\" or \"z.B.\"?\n",
    " * Should we keep punctuation information? Yes. For treating the examples above but also for any other kind of acronym.\n",
    " \n",
    "Let's see what TextBlob and spaCy do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Ich suche einen guten Arzt, z.B. so jemanden wie Dr. Karl-Heinz Schmitz.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ich' 'suche' 'einen' 'guten' 'Arzt' ',' 'z.B.' 'so' 'jemanden' 'wie' 'Dr.' 'Karl-Heinz' 'Schmitz' '.'\n"
     ]
    }
   ],
   "source": [
    "textblob_doc = TextBlob(text)    \n",
    "print(' '.join('\\'{w}\\''.format(w=t) for t in textblob_doc.tokens))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Ich' 'suche' 'einen' 'guten' 'Arzt' ',' 'z.B.' 'so' 'jemanden' 'wie' 'Dr.' 'Karl-Heinz' 'Schmitz' '.'\n"
     ]
    }
   ],
   "source": [
    "spacy_doc = spacy_nlp(text)    \n",
    "print(' '.join('\\'{w}\\''.format(w=t) for t in spacy_doc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Wie' 'funktioniert' 'das' 'mit' 'dem' '\"' 'Hand-Out' '\"' '?'\n",
      "'Wie' 'funktioniert' 'das' 'mit' 'dem' '``' 'Hand-Out' '''' '?'\n"
     ]
    }
   ],
   "source": [
    "text = \"Wie funktioniert das mit dem \\\"Hand-Out\\\"?\"\n",
    "spacy_doc = spacy_nlp(text)    \n",
    "print(' '.join('\\'{w}\\''.format(w=t) for t in spacy_doc)) \n",
    "\n",
    "textblob_doc = TextBlob(text)    \n",
    "print(' '.join('\\'{w}\\''.format(w=t) for t in textblob_doc.tokens))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Both tools provide decent tokenization capabilities. Let's check sentence splitting next.\n",
    "Sentence splitting (or paragraph splitting) divides a longer text document into larger units. Ideally, it should be able to distinguish headlines from the following sentences, not split on acronym punctuation and so so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Wie funktioniert das mit dem \"Hand-Out\"?' 'Können wir das mit de A.B.C. Methode lösen?'\n",
      "'Wie funktioniert das mit dem \"Hand-Out\"?' 'Können wir das mit de A.B.C.' 'Methode lösen?'\n"
     ]
    }
   ],
   "source": [
    "text = \"Wie funktioniert das mit dem \\\"Hand-Out\\\"? Können wir das mit de A.B.C. Methode lösen?\"\n",
    "spacy_doc = spacy_nlp(text)    \n",
    "print(' '.join('\\'{w}\\''.format(w=t) for t in spacy_doc.sents)) \n",
    "\n",
    "textblob_doc = TextBlob(text)    \n",
    "print(' '.join('\\'{w}\\''.format(w=t) for t in textblob_doc.sentences))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy wins as it correctly detects the ancronym and does not create a new sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech Tagging\n",
    "Part-of-speech Tagging assigns each of the words in a sequence (i.e. in most cases a sentence) a category like verb, noun, pronoun, adjective, and so on. Initially, this requires \n",
    "\n",
    "Textblob uses the [Penn Treebank tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), spacy uses the [TIGER Treebank tagset](http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/TIGERCorpus/annotation/index.html) and also cleverly maps tags to a [simplified, universal Tagest](http://www.petrovi.de/data/lrec.pdf). \n",
    "\n",
    "Why clever? First, such a reduced tag set is easier to grasp for somebody without a background in linguistitics. But it also simplifies the manual creation of rules based on POS-Tag patterns, as for instance often used in sentiment analysis.  \n",
    "\n",
    "\n",
    "Our example sentence will be \"Ich habe Kopfweh\" which translates to \"I have an headache\" in English.\n",
    "Let's first check how stable the tools are when the text ignores capitalization rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: ich \t tag: PRP\n",
      "word: habe \t tag: VB\n",
      "word: kopfweh \t tag: NN\n"
     ]
    }
   ],
   "source": [
    "textblob_doc = TextBlob('ich habe kopfweh')\n",
    "for word, tag in textblob_doc.tags:\n",
    "    print(\"word: %s \\t tag: %s\" % (word,tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good. Let's see how spaCy performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: ich \t coarse-tag: PRON \t fine-tag: PPER\n",
      "word: habe \t coarse-tag: AUX \t fine-tag: VAFIN\n",
      "word: kopfweh \t coarse-tag: ADJ \t fine-tag: ADJD\n"
     ]
    }
   ],
   "source": [
    "spacy_doc = spacy_nlp(u'ich habe kopfweh')\n",
    "for token in spacy_doc:\n",
    "    print('word: %s \\t coarse-tag: %s \\t fine-tag: %s' % (token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While _ich_ (english: I) and _habe_ (eng.: have) are tagged  correctly,  _kopfweh_ (eng.: head ache) was tagged as an adjective (`ADJD`) instead of a pronoun. \n",
    "\n",
    "Now let's use the proper case for every word and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Ich \t tag: PRP\n",
      "word: habe \t tag: VB\n",
      "word: Kopfweh \t tag: NN\n"
     ]
    }
   ],
   "source": [
    "textblob_doc = TextBlob('Ich habe Kopfweh')\n",
    "for word, tag in textblob_doc.tags:\n",
    "    print(\"word: %s \\t tag: %s\" % (word,tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good! Not suprising, since the first variant was also correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Ich \t coarse-tag: PRON \t fine-tag: PPER\n",
      "word: habe \t coarse-tag: AUX \t fine-tag: VAFIN\n",
      "word: Kopfweh \t coarse-tag: NOUN \t fine-tag: NN\n"
     ]
    }
   ],
   "source": [
    "spacy_doc = spacy_nlp(u'Ich habe Kopfweh')\n",
    "for token in spacy_doc:\n",
    "    print(\"word: %s \\t coarse-tag: %s \\t fine-tag: %s\" % (token.text, token.pos_, token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "All good now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun-phrase chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Könnte man etwas gegen meine Kopfschmerzen tun?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Könnte \t coarse-tag: VERB \t fine-tag: VMFIN\n",
      "word: man \t coarse-tag: PRON \t fine-tag: PIS\n",
      "word: etwas \t coarse-tag: ADV \t fine-tag: ADV\n",
      "word: gegen \t coarse-tag: ADP \t fine-tag: APPR\n",
      "word: meine \t coarse-tag: DET \t fine-tag: PPOSAT\n",
      "word: Kopfschmerzen \t coarse-tag: NOUN \t fine-tag: NN\n",
      "word: tun \t coarse-tag: VERB \t fine-tag: VVINF\n",
      "word: ? \t coarse-tag: PUNCT \t fine-tag: $.\n",
      "chunk: man \t\n",
      "chunk: meine Kopfschmerzen \t\n"
     ]
    }
   ],
   "source": [
    "spacy_doc = spacy_nlp(text)\n",
    "for token in spacy_doc:\n",
    "    print(\"word: %s \\t coarse-tag: %s \\t fine-tag: %s\" % (token.text, token.pos_, token.tag_))\n",
    "        \n",
    "# show noun chunks\n",
    "for chunk in spacy_doc.noun_chunks:\n",
    "    print(\"chunk: %s \\t\" % chunk.text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob_de import PatternParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Könnte \t tag: VB\n",
      "word: man \t tag: DT\n",
      "word: etwas \t tag: DT\n",
      "word: gegen \t tag: IN\n",
      "word: meine \t tag: VB\n",
      "word: Kopfschmerzen \t tag: NN\n",
      "word: tun \t tag: VB\n"
     ]
    }
   ],
   "source": [
    "textblob_doc = TextBlob(text)\n",
    "for word, tag in textblob_doc.tags:\n",
    "    print(\"word: %s \\t tag: %s\" % (word,tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for chunk in textblob_doc.noun_phrases:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per default, only noun_phrases that consist of two or more meaningful parts are displayed. \n",
    "Actually, the word kopfschmerzen is tagged correctly as NP which means Noun Phrase. But probably since meine is tagged incorrectly as a verb and not a determiner? we don't find a nounphrase. \n",
    "Investigating the lemma textblob assigns to this word, we will see below that it mistakes it for _mean_ instead of _mine_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob_de import PatternParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          WORD   TAG    CHUNK   ROLE   ID     PNP    LEMMA           \n",
      "                                                                     \n",
      "        Könnte   VB     VP      -      -      -      können          \n",
      "           man   DT     -       -      -      -      man             \n",
      "         etwas   DT     -       -      -      -      etwas           \n",
      "         gegen   IN     PP      -      -      -      gegen           \n",
      "         meine   VB     VP      -      -      -      meinen          \n",
      " Kopfschmerzen   NN     NP      -      -      -      kopfschmerzen   \n",
      "           tun   VB     VP      -      -      -      tun             \n",
      "             ?   .      -       -      -      -      ?               \n",
      "können\n",
      "man\n",
      "etwas\n",
      "gegen\n",
      "meinen\n",
      "Kopfschmerzen\n",
      "tun\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(text, parser=PatternParser(pprint=True, lemmata=True))\n",
    "blob.parse()\n",
    "    \n",
    "for t in blob.words.lemmatize():\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Könnte/Könnte man/man etwas/etwas gegen/gegen meine/meinen Kopfschmerzen/Kopfschmerzen tun/tun ?/?\n"
     ]
    }
   ],
   "source": [
    "# show lemmas produced by spaCy\n",
    "print(' '.join('{word}/{lemma}'.format(word=t.orth_, lemma=t.lemma_) for t in spacy_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=1.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"TextBlob ist richtig super\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"TextBlob ist nicht super\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-1.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"TextBlob ist super schlecht\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pluralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universities\n",
      "word: ich \t tag: iches\n",
      "word: habe \t tag: habes\n",
      "word: kopfweh \t tag: kopfwehs\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word(\"university\")\n",
    "print(w.pluralize())\n",
    "\n",
    "\n",
    "textblob_doc = TextBlob('ich habe kopfweh')\n",
    "for word, tag in textblob_doc.tags:\n",
    "    print(\"word: %s \\t tag: %s\" % (word, Word(word).pluralize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21771714593758457"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word vectors are attached to tokens and can be accessed via t.vector\n",
    "word1 = spacy_doc[2:3]\n",
    "word2 = spacy_doc[4:5]\n",
    "word1.similarity(word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wie      <--mo--- funktioniert\n",
      "funktioniert <-ROOT-- funktioniert\n",
      "das      <--sb--- funktioniert\n",
      "mit      <--mo--- funktioniert\n",
      "dem      <--nk--- Hand-Out\n",
      "\"        <--pnc-- Hand-Out\n",
      "Hand-Out <--nk--- mit\n",
      "\"        <-punct- Hand-Out\n",
      "?        <-punct- funktioniert\n",
      "Können   <-ROOT-- Können\n",
      "wir      <--sb--- Können\n",
      "das      <--oa--- lösen\n",
      "mit      <--mo--- lösen\n",
      "de       <--pnc-- A.B.C.\n",
      "A.B.C.   <--nk--- mit\n",
      "Methode  <--nk--- mit\n",
      "lösen    <--oc--- Können\n",
      "?        <-punct- Können\n"
     ]
    }
   ],
   "source": [
    "# show dependency arcs\n",
    "print('\\n'.join('{child:<8} <{label:-^7} {head}'.format(child=t.orth_, label=t.dep_, head=t.head.orth_) for t in spacy_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hand-Out\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# show named entities\n",
    "for ent in spacy_doc.ents:\n",
    "    print(ent.text)\n",
    "      \n",
    "    \n",
    "print([w[0] for w in textblob_doc.tags if w[1] == u'NNP'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Links\n",
    "* **English Only?**: [TextAnalysis Api](http://textanalysisonline.com/): TextAnalysis Api provides customized Text Analysis or Text Mining Services like tokenization, POS-Tagging, stemming, lemmatization, chunking, parsing, sentence segmentation, gammar checking, sentiment analysis, text summarization, text classification and other text analysis tasks. It stands on the giant shoulders of NLP Tools, such as NLTK, TextBlob, Pattern, MBSP and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
