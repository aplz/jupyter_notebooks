{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "We'll only need the [Python Elasticsearch Client](https://elasticsearch-py.readthedocs.io/en/master/). \n",
    "\n",
    "`pip install elasticsearch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. You don't need to have an active instance of elastic running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using elasticsearch analyzers as tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Die junge Informatikerin Katie Bouman machte die historische Aufnahme \"\\\n",
    "       \" eines schwarzen Lochs möglich.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the German analyzer\n",
    "The German Analyzer includes conversion to lowercase, stemming and stop word filtering. \n",
    "See [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#german-analyzer) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'jung', 'start_offset': 4, 'end_offset': 9, 'type': '<ALPHANUM>', 'position': 1}\n",
      "{'token': 'informatikerin', 'start_offset': 10, 'end_offset': 24, 'type': '<ALPHANUM>', 'position': 2}\n",
      "{'token': 'kati', 'start_offset': 25, 'end_offset': 30, 'type': '<ALPHANUM>', 'position': 3}\n",
      "{'token': 'bouman', 'start_offset': 31, 'end_offset': 37, 'type': '<ALPHANUM>', 'position': 4}\n",
      "{'token': 'macht', 'start_offset': 38, 'end_offset': 44, 'type': '<ALPHANUM>', 'position': 5}\n",
      "{'token': 'historisch', 'start_offset': 49, 'end_offset': 60, 'type': '<ALPHANUM>', 'position': 7}\n",
      "{'token': 'aufnahm', 'start_offset': 61, 'end_offset': 69, 'type': '<ALPHANUM>', 'position': 8}\n",
      "{'token': 'schwarz', 'start_offset': 77, 'end_offset': 86, 'type': '<ALPHANUM>', 'position': 10}\n",
      "{'token': 'loch', 'start_offset': 87, 'end_offset': 92, 'type': '<ALPHANUM>', 'position': 11}\n",
      "{'token': 'moglich', 'start_offset': 93, 'end_offset': 100, 'type': '<ALPHANUM>', 'position': 12}\n"
     ]
    }
   ],
   "source": [
    "tokens = es.indices.analyze(\n",
    "  body={\"analyzer\": \"german\",\n",
    "        \"text\": text})['tokens']\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the German analyzer w/o stemming\n",
    "Since stemming is usually rather agressive and we may loose a lot of semantic interpretability, we can exclude it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token': 'junge', 'start_offset': 4, 'end_offset': 9, 'type': '<ALPHANUM>', 'position': 1}\n",
      "{'token': 'informatikerin', 'start_offset': 10, 'end_offset': 24, 'type': '<ALPHANUM>', 'position': 2}\n",
      "{'token': 'katie', 'start_offset': 25, 'end_offset': 30, 'type': '<ALPHANUM>', 'position': 3}\n",
      "{'token': 'bouman', 'start_offset': 31, 'end_offset': 37, 'type': '<ALPHANUM>', 'position': 4}\n",
      "{'token': 'machte', 'start_offset': 38, 'end_offset': 44, 'type': '<ALPHANUM>', 'position': 5}\n",
      "{'token': 'historische', 'start_offset': 49, 'end_offset': 60, 'type': '<ALPHANUM>', 'position': 7}\n",
      "{'token': 'aufnahme', 'start_offset': 61, 'end_offset': 69, 'type': '<ALPHANUM>', 'position': 8}\n",
      "{'token': 'schwarzen', 'start_offset': 77, 'end_offset': 86, 'type': '<ALPHANUM>', 'position': 10}\n",
      "{'token': 'lochs', 'start_offset': 87, 'end_offset': 92, 'type': '<ALPHANUM>', 'position': 11}\n",
      "{'token': 'möglich', 'start_offset': 93, 'end_offset': 100, 'type': '<ALPHANUM>', 'position': 12}\n"
     ]
    }
   ],
   "source": [
    "tokens = es.indices.analyze(\n",
    "  body={\"tokenizer\": \"standard\", \"filter\": [\"lowercase\", {\"type\": \"stop\", \"stopwords\": \"_german_\"}],\n",
    "        \"text\": text})['tokens']\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
